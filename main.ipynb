{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G21SRR-YOJvi"
   },
   "source": [
    "# Information about paper and code\n",
    "\n",
    "## Code Authors\n",
    "Erce Guder - Adnan Harun Dogan\n",
    "\n",
    "## Paper Name:\n",
    "FEW-SHOT CROSS-DOMAIN IMAGE GENERATION VIA INFERENCE-TIME LATENT-CODE LEARNING (ICLR2023) https://openreview.net/pdf?id=sCYXJr3QJM8\n",
    "\n",
    "## Paper Summary\n",
    "_\"Can a GAN trained on a single large-scale source dataset be adapted to multiple target domains containing very few examples without re-training the pre-trained source generator?\"_\n",
    "\n",
    "The goal of the paper is to learn a latent-generation network (during inference stage) that maps random Gaussian noise to latents in the W space (style) of a pre-trained StyleGAN2 **without updating the generator** (we don't want to overfit / forget rich prior knowledge) such that the generator can sample from the target domain. There are two loss functions used to ensure that the samples belong to target domain:\n",
    "\n",
    "  - Adversarial loss\n",
    "  - Style Loss (Content Loss / VGG-Loss)\n",
    "\n",
    "## Datasets: \n",
    "We shall take StyleGAN2 checkpoints trained on:\n",
    "* *Flickr Faces HQ (FFHQ)* dataset. Our target domains will be\n",
    "\n",
    "  - FFHQ-Babies,\n",
    "  - FFHQ-Sunglasses, \n",
    "  - Face sketches, \n",
    "  - Emoji faces from bitmoji.com, \n",
    "  - Portrait paintings from the artistic faces dataset.\n",
    "\n",
    "* *LSUN Church* as a source domain and adapt to\n",
    "  - the haunted houses, \n",
    "  - Van Goh’s house paintings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgWn1XnkOOkN"
   },
   "source": [
    "# Hyper-parameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the authors of this paper also did, we took checkpoints that were presented by StyleGAN2 repositories. So, we are not sure if we should present the hyper-parameters of StyleGAN2.\n",
    "\n",
    "Nevertheless, the hyper-parameters of StyleGAN2:\n",
    " - Output size of 256x256 pixels (generator & discriminator)\n",
    " - W dimension of 512 (generator)\n",
    " - Number of layers mapping Z to W: 8 (generator)\n",
    "\n",
    "The latent generation network is decided to be (mentioned in the paper):\n",
    " - 3-Layer MLP with\n",
    " - ReLU activations\n",
    "\n",
    "Rest of the hyper-parameters are as follows:\n",
    " - Learning rate for discriminator & latent generation network: 5e-4\n",
    " - Optimizer for discriminator & latent generation network:\n",
    "     - Adam with betas (0.0, 0.99)\n",
    " - Batch size: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8TImfXqOOom"
   },
   "source": [
    "# Training and saving a model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "error",
     "timestamp": 1684781538049,
     "user": {
      "displayName": "Adnan Harun DOĞAN",
      "userId": "10197058931241530903"
     },
     "user_tz": -180
    },
    "id": "tcmbuzZiiP5c",
    "outputId": "1ab61918-a287-4484-deb4-de2e2988f3d9"
   },
   "outputs": [],
   "source": [
    "from model import Generator, Discriminator\n",
    "from latent_learner import LatentLearner\n",
    "from dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "import loss\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_samples(x_1, x_2, generator, latent_learner, iter_):\n",
    "    os.makedirs(\"samples\", exist_ok=True)\n",
    "\n",
    "    noise = torch.cat([x_1, x_2], axis=-1)\n",
    "    # Map to latent\n",
    "    w = latent_learner(noise)\n",
    "\n",
    "    # Pass through Generator\n",
    "    samples, _ = generator([w])\n",
    "\n",
    "    # Save for later examination\n",
    "    torchvision.utils.save_image(\n",
    "        samples.detach(),\n",
    "        f\"samples/samples_{iter_}.png\",\n",
    "        nrow=1,\n",
    "        normalize=True,\n",
    "        range=(-1, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "error",
     "timestamp": 1684781538049,
     "user": {
      "displayName": "Adnan Harun DOĞAN",
      "userId": "10197058931241530903"
     },
     "user_tz": -180
    },
    "id": "tcmbuzZiiP5c",
    "outputId": "1ab61918-a287-4484-deb4-de2e2988f3d9"
   },
   "outputs": [],
   "source": [
    "def disable_grad(model):\n",
    "    for _, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def train(device, max_iters=150):\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "    generator = Generator(size=256, style_dim=512, n_mlp=8).to(device)\n",
    "    discriminator = Discriminator(size=256).to(device)\n",
    "    latent_learner = LatentLearner().to(device)\n",
    "\n",
    "    vgg = torchvision.models.vgg19(weights='IMAGENET1K_V1').features.to(device).eval()\n",
    "\n",
    "    # No need for gradients on the parameters of these\n",
    "    disable_grad(generator)\n",
    "    disable_grad(vgg)\n",
    "\n",
    "    # Take sub-networks from the vgg, later used to compute style loss\n",
    "    subnetworks = loss.subnetworks(vgg, max_layers=5)\n",
    "\n",
    "    # Garbage collection\n",
    "    del vgg\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load checkpoint and weights\n",
    "    ckpt = torch.load(\"550000.pt\")\n",
    "\n",
    "    generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
    "    generator.eval()\n",
    "\n",
    "    discriminator.load_state_dict(ckpt[\"d\"])\n",
    "\n",
    "    # Initialize optimizers (no optimizer for generator :)\n",
    "    disc_opt = torch.optim.Adam(\n",
    "        discriminator.parameters(),\n",
    "        lr = 5e-4,\n",
    "        betas = (0.0, 0.99)\n",
    "    )\n",
    "    latent_learner_opt = torch.optim.Adam(\n",
    "        latent_learner.parameters(),\n",
    "        lr = 5e-4,\n",
    "        betas = (0.0, 0.99)\n",
    "    )\n",
    "\n",
    "    # Simple transformation pipeline\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((256, 256)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Create simple dataset & loader\n",
    "    x_1 = torch.randn(10, 14, 512, device=device)\n",
    "    x_2 = torch.randn(10, 14, 512, device=device)\n",
    "    #x = torch.randn(10, 1024, device=device)\n",
    "\n",
    "    dataset = Dataset(path=\"./babies\", device=device, transforms=transforms)\n",
    "\n",
    "    bar = tqdm(range(max_iters))\n",
    "\n",
    "    d_loss_hist = list()\n",
    "    g_loss_hist = list()\n",
    "\n",
    "    # 150 iterations\n",
    "    for idx in bar:\n",
    "        i = np.random.choice(10, size=4, replace=False)\n",
    "        imgs = dataset[i]\n",
    "#        noise = x[i]\n",
    "\n",
    "        idx_1 = np.random.choice(10, size=imgs.shape[0], replace=False)\n",
    "        idx_2 = np.random.choice(10, size=imgs.shape[0], replace=False)\n",
    "\n",
    "        x = torch.cat([x_1[idx_1], x_2[idx_2]], axis=-1)\n",
    "\n",
    "        ##### Adversarial Loss ##### \n",
    "        # first forward pass\n",
    "        w = latent_learner(x)\n",
    "        samples, _ = generator([w], input_is_latent=True)\n",
    "\n",
    "        real_scores = discriminator(imgs)\n",
    "        fake_scores = discriminator(samples)\n",
    "\n",
    "#        print(f\"fake_scores: {fake_scores.view(-1).cpu()}, real_scores: {real_scores.view(-1).cpu()}\")\n",
    "\n",
    "        d_loss = loss.d_logistic_loss(real_scores, fake_scores)\n",
    "\n",
    "        # optimization step on discriminator\n",
    "        disc_opt.zero_grad()\n",
    "        d_loss.backward()\n",
    "        disc_opt.step()\n",
    "\n",
    "        # second forward pass (needed)\n",
    "        w = latent_learner(x)\n",
    "        samples, _ = generator([w], input_is_latent=True)\n",
    "\n",
    "        fake_scores = discriminator(samples)\n",
    "\n",
    "        g_loss = 5 * (1 - idx/max_iters) * loss.g_nonsaturating_loss(fake_scores)\n",
    "        #g_loss = loss.g_nonsaturating_loss(fake_scores)\n",
    "\n",
    "        # optimization step on latent learner\n",
    "        latent_learner_opt.zero_grad()\n",
    "        g_loss.backward()\n",
    "        latent_learner_opt.step()\n",
    "\n",
    "#        bar.set_description(f\"d_loss: {d_loss.cpu():.2f}, g_loss: {g_loss.cpu():.2f}\")\n",
    "\n",
    "        d_loss_hist.append(d_loss.detach().cpu())\n",
    "        g_loss_hist.append(g_loss.detach().cpu())\n",
    "#        continue\n",
    "\n",
    "        ##### Style Loss #####\n",
    "        img_idx = np.random.choice(10, size=5, replace=False)\n",
    "        imgs = dataset[img_idx]\n",
    "\n",
    "        idx_1 = np.random.choice(10, size=imgs.shape[0], replace=False)\n",
    "        #idx_2 = np.random.choice(10, size=imgs.shape[0], replace=False)\n",
    "\n",
    "        x = torch.cat([x_1[idx_1], x_2[img_idx]], axis=-1)\n",
    "\n",
    "        w = latent_learner(x)\n",
    "        samples, _ = generator([w], input_is_latent=True)\n",
    "\n",
    "        style_loss = 0.0\n",
    "\n",
    "        for i, img in enumerate(imgs):\n",
    "            style_loss += 50 * loss.style_loss(subnetworks, img, samples[i])\n",
    "\n",
    "        # take mean over the batch\n",
    "        style_loss /= len(imgs)\n",
    "\n",
    "        # optimization step on latent learner\n",
    "        latent_learner_opt.zero_grad()\n",
    "        style_loss.backward()\n",
    "        latent_learner_opt.step()\n",
    "\n",
    "        bar.set_description(f\"d_loss: {d_loss.cpu():.2f}, g_loss: {g_loss.cpu():.2f}, style_loss: {style_loss:.2f}\")\n",
    "\n",
    "        if (idx+1) % 50 == 0:\n",
    "            save_samples(x_1, x_2, generator, latent_learner, idx+1)\n",
    "\n",
    "    return torch.cat([x_1, x_2], axis=-1), latent_learner, generator, d_loss_hist, g_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise, latent_learner, generator, d_loss_hist, g_loss_hist = train(device, max_iters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_loss_hist, label=\"d_loss\")\n",
    "plt.plot(g_loss_hist, label=\"g_loss\")\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"ckpts\", exist_ok=True)\n",
    "\n",
    "torch.save(noise, \"ckpts/noise.pt\")\n",
    "torch.save(latent_learner.state_dict(), \"ckpts/latent_learner.pt\")\n",
    "torch.save(generator.state_dict(), \"ckpts/generator.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orRKWKuIOOrG"
   },
   "source": [
    "# Loading a pre-trained model and computing qualitative samples/outputs from that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(size=256, style_dim=512, n_mlp=8).to(device)\n",
    "generator.load_state_dict(torch.load(\"ckpts/generator.pt\"))\n",
    "generator.eval()\n",
    "\n",
    "latent_learner = LatentLearner().to(device)\n",
    "latent_learner.load_state_dict(torch.load(\"ckpts/latent_learner.pt\"))\n",
    "\n",
    "noise = torch.load(\"ckpts/noise.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Map to latent\n",
    "    w = latent_learner(noise)\n",
    "\n",
    "    # Pass through Generator\n",
    "    samples, _ = generator([w])\n",
    "\n",
    "    # Save for later examination\n",
    "    torchvision.utils.save_image(\n",
    "        samples.detach(),\n",
    "        \"samples.png\",\n",
    "        nrow=1,\n",
    "        normalize=True,\n",
    "        range=(-1, 1),\n",
    "    )\n",
    "\n",
    "    from PIL import Image\n",
    "\n",
    "    display(Image.open(\"samples.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is obvious that the generator collapsed, and the results are not baby images.**\n",
    "\n",
    "We believe that this is a  direct consequence of us not being able to fully understand the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMtFS0H4B8FkdU6YVuYeyRi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
